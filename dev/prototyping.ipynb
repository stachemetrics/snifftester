{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15e239d6",
   "metadata": {},
   "source": [
    "# basic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec9cffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /usr/local/python/3.12.1/lib/python3.12/site-packages (1.2.1)\n",
      "Collecting google-genai\n",
      "  Downloading google_genai-1.57.0-py3-none-any.whl.metadata (53 kB)\n",
      "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai) (4.11.0)\n",
      "Requirement already satisfied: google-auth<3.0.0,>=2.46.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth[requests]<3.0.0,>=2.46.0->google-genai) (2.47.0)\n",
      "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.9.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-genai) (2.12.5)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.28.1 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai) (2.32.5)\n",
      "Collecting tenacity<9.2.0,>=8.2.3 (from google-genai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /home/codespace/.local/lib/python3.12/site-packages (from google-genai) (4.15.0)\n",
      "Collecting distro<2,>=1.7.0 (from google-genai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from google-genai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.11)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai) (4.9.1)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.9.0->google-genai) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.12/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.5.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from rsa<5,>=3.1.4->google-auth<3.0.0,>=2.46.0->google-auth[requests]<3.0.0,>=2.46.0->google-genai) (0.6.1)\n",
      "Downloading google_genai-1.57.0-py3-none-any.whl (713 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m713.3/713.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Installing collected packages: websockets, tenacity, distro, google-genai\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [google-genai][0m [google-genai]\n",
      "\u001b[1A\u001b[2KSuccessfully installed distro-1.9.0 google-genai-1.57.0 tenacity-9.1.2 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58b5b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "assert api_key is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "09ef8dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "client = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d527ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_TEXT = \"\"\"\n",
    "The impact of artificial intelligence on scientific research has been profound. According to a study \n",
    "published in Nature (Smith et al., 2023, \"AI-Driven Discovery in Materials Science\", \n",
    "DOI: 10.1038/s41586-023-06221-2), machine learning models have accelerated materials discovery by 50x.\n",
    "\n",
    "This trend is also reflected in social media discussions. A viral Twitter thread by @AIResearcher \n",
    "(https://twitter.com/AIResearcher/status/1234567890) garnered over 50,000 likes discussing how \n",
    "GPT-4 is being used in drug discovery pipelines.\n",
    "\n",
    "The New York Times reported on this phenomenon in their article \"The AI Revolution in Labs\" \n",
    "(https://www.nytimes.com/2023/08/15/science/ai-laboratory-research.html), noting that major \n",
    "pharmaceutical companies are investing billions.\n",
    "\n",
    "A Reddit discussion on r/MachineLearning (https://reddit.com/r/MachineLearning/comments/abc123) \n",
    "with 2,400 upvotes debated the reproducibility concerns raised in the original Nature paper.\n",
    "\n",
    "For a technical deep-dive, the arXiv preprint \"Transformer Architectures for Scientific Computing\" \n",
    "(arXiv:2401.12345) provides implementation details. The official documentation at \n",
    "https://docs.example-ai-tool.com/guide has step-by-step tutorials.\n",
    "\n",
    "Personal blog post by Dr. Jane Chen (https://janechenml.substack.com/p/ai-in-science-2024) \n",
    "offers a practitioner's perspective with 15,000 views.\n",
    "\n",
    "The CDC guidelines (https://www.cdc.gov/ai-health/guidelines.html) now include recommendations \n",
    "for AI-assisted diagnostics.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb03c0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACTION_PROMPT = \"\"\"\n",
    "You are a reference extraction system. Analyze the following text and extract ALL references, citations, and sources mentioned.\n",
    "\n",
    "For each reference found, provide:\n",
    "1. raw_text: The exact text/citation as it appears\n",
    "2. type: One of: academic_journal, academic_preprint, news_media, government_official, social_media, personal_blog, documentation, unknown\n",
    "3. url: The URL if present (null if not)\n",
    "4. doi: The DOI if present (null if not)\n",
    "5. platform: The platform/source (e.g., \"Nature\", \"Twitter\", \"arXiv\", \"Reddit\", \"NYTimes\", etc.)\n",
    "6. engagement_signals: Any mentioned metrics like views, likes, upvotes (null if not mentioned)\n",
    "\n",
    "Return the results as a JSON array. Be thorough - extract every reference, even implicit ones.\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "---\n",
    "{text}\n",
    "---\n",
    "\n",
    "Return ONLY valid JSON, no markdown formatting or explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1d00c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def extract_references(text: str) -> list[dict]:\n",
    "    \"\"\"Extract references from text using Gemini.\"\"\"\n",
    "    \n",
    "    prompt = EXTRACTION_PROMPT.format(text=text)\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.0-flash-exp\",\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    # Parse the JSON response\n",
    "    try:\n",
    "        response_text = response.text.strip()\n",
    "        # Clean up markdown code blocks if present\n",
    "        if response_text.startswith(\"```json\"):\n",
    "            response_text = response_text[7:]\n",
    "        if response_text.startswith(\"```\"):\n",
    "            response_text = response_text[3:]\n",
    "        if response_text.endswith(\"```\"):\n",
    "            response_text = response_text[:-3]\n",
    "        \n",
    "        return json.loads(response_text.strip())\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing JSON: {e}\")\n",
    "        print(f\"Raw response: {response.text}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5579bc7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 references:\n",
      "\n",
      "[\n",
      "  {\n",
      "    \"raw_text\": \"Nature (Smith et al., 2023, \\\"AI-Driven Discovery in Materials Science\\\", DOI: 10.1038/s41586-023-06221-2)\",\n",
      "    \"type\": \"academic_journal\",\n",
      "    \"url\": null,\n",
      "    \"doi\": \"10.1038/s41586-023-06221-2\",\n",
      "    \"platform\": \"Nature\",\n",
      "    \"engagement_signals\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"@AIResearcher (https://twitter.com/AIResearcher/status/1234567890)\",\n",
      "    \"type\": \"social_media\",\n",
      "    \"url\": \"https://twitter.com/AIResearcher/status/1234567890\",\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"Twitter\",\n",
      "    \"engagement_signals\": \"50,000 likes\"\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"The New York Times reported on this phenomenon in their article \\\"The AI Revolution in Labs\\\" (https://www.nytimes.com/2023/08/15/science/ai-laboratory-research.html)\",\n",
      "    \"type\": \"news_media\",\n",
      "    \"url\": \"https://www.nytimes.com/2023/08/15/science/ai-laboratory-research.html\",\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"NYTimes\",\n",
      "    \"engagement_signals\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Reddit discussion on r/MachineLearning (https://reddit.com/r/MachineLearning/comments/abc123)\",\n",
      "    \"type\": \"social_media\",\n",
      "    \"url\": \"https://reddit.com/r/MachineLearning/comments/abc123\",\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"Reddit\",\n",
      "    \"engagement_signals\": \"2,400 upvotes\"\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"arXiv preprint \\\"Transformer Architectures for Scientific Computing\\\" (arXiv:2401.12345)\",\n",
      "    \"type\": \"academic_preprint\",\n",
      "    \"url\": null,\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"arXiv\",\n",
      "    \"engagement_signals\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"https://docs.example-ai-tool.com/guide\",\n",
      "    \"type\": \"documentation\",\n",
      "    \"url\": \"https://docs.example-ai-tool.com/guide\",\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"docs.example-ai-tool.com\",\n",
      "    \"engagement_signals\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Personal blog post by Dr. Jane Chen (https://janechenml.substack.com/p/ai-in-science-2024)\",\n",
      "    \"type\": \"personal_blog\",\n",
      "    \"url\": \"https://janechenml.substack.com/p/ai-in-science-2024\",\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"substack.com\",\n",
      "    \"engagement_signals\": \"15,000 views\"\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"The CDC guidelines (https://www.cdc.gov/ai-health/guidelines.html)\",\n",
      "    \"type\": \"government_official\",\n",
      "    \"url\": \"https://www.cdc.gov/ai-health/guidelines.html\",\n",
      "    \"doi\": null,\n",
      "    \"platform\": \"cdc.gov\",\n",
      "    \"engagement_signals\": null\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "references = extract_references(SAMPLE_TEXT)\n",
    "print(f\"Found {len(references)} references:\\n\")\n",
    "print(json.dumps(references, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fe875d",
   "metadata": {},
   "source": [
    "reflections:\n",
    "- data structure encourages hallucination - have not defined web search as a tool, so engagement_signals can be hallucinated.\n",
    "- may want to start with a digital heuristic - EEAT, experience expertise authorativeness trustworthiness\n",
    "- will want to log tokens in, tokens out, latency, to enable evaluation.\n",
    "- may want to start an examples folder with real outputs we've evaluated (to enable quality ratings) - or find another example elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7a82d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e05f7ccf",
   "metadata": {},
   "source": [
    "# Iteration 2\n",
    "Get logging, improve prompt, provide outputs for UX elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6789f166",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ generate_with_logging defined\n",
      "✅ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "@dataclass\n",
    "class LLMLog:\n",
    "    timestamp: str\n",
    "    model: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_ms: int\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "# Store logs for this session\n",
    "llm_logs: list[LLMLog] = []\n",
    "\n",
    "def generate_with_logging(prompt: str, model: str = \"gemini-2.0-flash-exp\") -> tuple[str, LLMLog]:\n",
    "    \"\"\"Generate content and log token usage + latency.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    latency_ms = int((time.time() - start_time) * 1000)\n",
    "    \n",
    "    usage = response.usage_metadata\n",
    "    log = LLMLog(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model=model,\n",
    "        prompt_tokens=usage.prompt_token_count,\n",
    "        completion_tokens=usage.candidates_token_count,\n",
    "        total_tokens=usage.total_token_count,\n",
    "        latency_ms=latency_ms\n",
    "    )\n",
    "    \n",
    "    llm_logs.append(log)\n",
    "    \n",
    "    return response.text, log\n",
    "\n",
    "print(\"✅ generate_with_logging defined\")\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51d5b677",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('example_question.txt',mode='r') as file:\n",
    "    EXAMPLE_QUESTION = file.read()\n",
    "\n",
    "with open('example_answer.txt',mode='r') as file:\n",
    "    EXAMPLE_ANSWER = file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de49e09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1510c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data structures defined\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class SourceType(str, Enum):\n",
    "    ACADEMIC_JOURNAL = \"academic_journal\"\n",
    "    ACADEMIC_PREPRINT = \"academic_preprint\"\n",
    "    NEWS_MEDIA = \"news_media\"\n",
    "    GOVERNMENT = \"government\"\n",
    "    SOCIAL_MEDIA = \"social_media\"\n",
    "    PERSONAL_BLOG = \"personal_blog\"\n",
    "    DOCUMENTATION = \"documentation\"\n",
    "    COMMERCIAL = \"commercial\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class ExtractedReference:\n",
    "    \"\"\"Stage 1 output - extracted from text only, no inference.\"\"\"\n",
    "    raw_text: str\n",
    "    url: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    arxiv_id: Optional[str] = None\n",
    "    source_type: SourceType = SourceType.UNKNOWN\n",
    "    platform_name: Optional[str] = None\n",
    "    # Only what's explicitly in text\n",
    "    stated_author: Optional[str] = None\n",
    "    stated_date: Optional[str] = None\n",
    "    stated_metrics: Optional[str] = None  # e.g., \"2,400 upvotes\" as string\n",
    "\n",
    "@dataclass \n",
    "class CRAAPScore:\n",
    "    \"\"\"Stage 2 output - requires verification.\"\"\"\n",
    "    score: int  # 1-5\n",
    "    evidence: str  # Why this score\n",
    "    \n",
    "@dataclass\n",
    "class VerifiedReference:\n",
    "    \"\"\"Stage 2 output - after search/verification.\"\"\"\n",
    "    reference: ExtractedReference\n",
    "    url_accessible: Optional[bool] = None\n",
    "    currency: Optional[CRAAPScore] = None\n",
    "    relevance: Optional[CRAAPScore] = None\n",
    "    authority: Optional[CRAAPScore] = None\n",
    "    accuracy: Optional[CRAAPScore] = None\n",
    "    purpose: Optional[CRAAPScore] = None\n",
    "    overall_score: Optional[float] = None\n",
    "    verification_notes: str = \"\"\n",
    "\n",
    "@dataclass\n",
    "class AnalysisInput:\n",
    "    \"\"\"Full context for analysis.\"\"\"\n",
    "    question: str\n",
    "    answer: str\n",
    "    context: dict = field(default_factory=dict)  # e.g., {\"user_location\": \"AU\", \"domain\": \"healthcare\"}\n",
    "\n",
    "print(\"✅ Data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29f5632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ extract_references_v2 defined\n"
     ]
    }
   ],
   "source": [
    "EXTRACTION_PROMPT_V2 = \"\"\"\n",
    "You are a reference extraction system. Extract ALL references from an AI-generated answer.\n",
    "\n",
    "CONTEXT:\n",
    "- User's question: {question}\n",
    "- User's context: {context}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "Extract every reference, citation, URL, or source mentioned in the answer.\n",
    "Only extract information EXPLICITLY stated. Use null for anything not directly written.\n",
    "\n",
    "For each reference provide:\n",
    "- raw_text: Exact citation as written\n",
    "- url: URL if present, null otherwise\n",
    "- doi: DOI if present, null otherwise  \n",
    "- arxiv_id: arXiv ID if present, null otherwise\n",
    "- source_type: One of: academic_journal, academic_preprint, news_media, government, social_media, personal_blog, documentation, commercial, unknown\n",
    "- platform_name: Specific platform/publication name\n",
    "- stated_author: Author name if explicitly written, null otherwise\n",
    "- stated_date: Publication date/year if written, null otherwise\n",
    "- stated_metrics: Any engagement metrics as written (e.g., \"50k views\"), null otherwise\n",
    "\n",
    "ANSWER TO ANALYZE:\n",
    "---\n",
    "{answer}\n",
    "---\n",
    "\n",
    "Return ONLY a JSON array. Do not hallucinate - null for anything not explicit.\n",
    "\"\"\"\n",
    "\n",
    "def extract_references_v2(input_data: AnalysisInput) -> tuple[list[dict], LLMLog]:\n",
    "    \"\"\"Extract references with question context.\"\"\"\n",
    "    \n",
    "    prompt = EXTRACTION_PROMPT_V2.format(\n",
    "        question=input_data.question,\n",
    "        context=json.dumps(input_data.context),\n",
    "        answer=input_data.answer\n",
    "    )\n",
    "    \n",
    "    response_text, log = generate_with_logging(prompt)\n",
    "    \n",
    "    try:\n",
    "        clean_text = response_text.strip()\n",
    "        if clean_text.startswith(\"```json\"):\n",
    "            clean_text = clean_text[7:]\n",
    "        if clean_text.startswith(\"```\"):\n",
    "            clean_text = clean_text[3:]\n",
    "        if clean_text.endswith(\"```\"):\n",
    "            clean_text = clean_text[:-3]\n",
    "        \n",
    "        return json.loads(clean_text.strip()), log\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: {e}\\nRaw: {response_text}\")\n",
    "        return [], log\n",
    "\n",
    "print(\"✅ extract_references_v2 defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "27a46e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ verify_reference_with_search defined\n"
     ]
    }
   ],
   "source": [
    "CRAAP_VERIFICATION_PROMPT = \"\"\"\n",
    "You are a source verification system evaluating references for a specific user question.\n",
    "\n",
    "USER QUESTION: {question}\n",
    "USER CONTEXT: {context}\n",
    "\n",
    "REFERENCE TO VERIFY:\n",
    "{reference_json}\n",
    "\n",
    "Using web search, evaluate this reference on the CRAAP framework. For each dimension, provide:\n",
    "- score: 1-5 (1=poor, 5=excellent)\n",
    "- evidence: Specific reasons for the score\n",
    "\n",
    "EVALUATION CRITERIA:\n",
    "\n",
    "**Currency (1-5):**\n",
    "- When was this published/updated?\n",
    "- Is the information current enough for the topic?\n",
    "- Are there more recent sources available?\n",
    "\n",
    "**Relevance (1-5):**\n",
    "- Does this source address the user's actual question?\n",
    "- Is it geographically relevant? (User is in {user_location})\n",
    "- Is it the right depth/audience level?\n",
    "\n",
    "**Authority (1-5):**\n",
    "- Who is the author? What are their credentials?\n",
    "- Is the publisher/platform reputable for this topic?\n",
    "- Is there institutional backing?\n",
    "\n",
    "**Accuracy (1-5):**\n",
    "- Can the claims be verified elsewhere?\n",
    "- Does the source cite its own references?\n",
    "- Is it peer-reviewed or edited?\n",
    "\n",
    "**Purpose (1-5):**\n",
    "- Why does this source exist? (inform/persuade/sell/entertain)\n",
    "- Is there obvious bias or conflict of interest?\n",
    "- Is it trying to sell something?\n",
    "\n",
    "Also check:\n",
    "- url_accessible: Can the URL be reached? (true/false/null if no URL)\n",
    "\n",
    "Return JSON:\n",
    "{{\n",
    "  \"url_accessible\": true/false/null,\n",
    "  \"currency\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"relevance\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"authority\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"accuracy\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"purpose\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"overall_score\": 1.0-5.0,\n",
    "  \"verification_notes\": \"Key findings from search\"\n",
    "}}\n",
    "\n",
    "Return ONLY valid JSON.\n",
    "\"\"\"\n",
    "\n",
    "def verify_reference_with_search(\n",
    "    reference: dict, \n",
    "    question: str, \n",
    "    context: dict,\n",
    "    model: str = \"gemini-2.0-flash-exp\"\n",
    ") -> tuple[dict, LLMLog]:\n",
    "    \"\"\"Verify a single reference using Gemini with Google Search.\"\"\"\n",
    "    \n",
    "    prompt = CRAAP_VERIFICATION_PROMPT.format(\n",
    "        question=question,\n",
    "        context=json.dumps(context),\n",
    "        reference_json=json.dumps(reference, indent=2),\n",
    "        user_location=context.get(\"user_location\", \"unknown\")\n",
    "    )\n",
    "    \n",
    "    # Enable Google Search grounding\n",
    "    from google.genai import types\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=[types.Tool(google_search=types.GoogleSearch())]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    latency_ms = int((time.time() - start_time) * 1000)\n",
    "    \n",
    "    usage = response.usage_metadata\n",
    "    log = LLMLog(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model=model,\n",
    "        prompt_tokens=usage.prompt_token_count,\n",
    "        completion_tokens=usage.candidates_token_count,\n",
    "        total_tokens=usage.total_token_count,\n",
    "        latency_ms=latency_ms\n",
    "    )\n",
    "    llm_logs.append(log)\n",
    "    \n",
    "    try:\n",
    "        clean_text = response.text.strip()\n",
    "        if clean_text.startswith(\"```json\"):\n",
    "            clean_text = clean_text[7:]\n",
    "        if clean_text.startswith(\"```\"):\n",
    "            clean_text = clean_text[3:]\n",
    "        if clean_text.endswith(\"```\"):\n",
    "            clean_text = clean_text[:-3]\n",
    "        \n",
    "        return json.loads(clean_text.strip()), log\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: {e}\\nRaw: {response.text}\")\n",
    "        return {}, log\n",
    "\n",
    "print(\"✅ verify_reference_with_search defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "981f59d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: EXTRACTION ===\n",
      "Tokens: 5678 | Latency: 10661ms\n",
      "Found 16 references\n",
      "[\n",
      "  {\n",
      "    \"raw_text\": \"iTnews\",\n",
      "    \"url\": \"https://www.itnews.com.au/feature/how-australian-businesses-can-overcome-the-struggle-to-move-ai-from-hype-to-reality-618510\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"news_media\",\n",
      "    \"platform_name\": \"iTnews\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Substack\",\n",
      "    \"url\": \"https://futureofprospecting.substack.com/p/the-ai-threat-that-could-break-salesforce\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"personal_blog\",\n",
      "    \"platform_name\": \"Substack\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"The Conversation\",\n",
      "    \"url\": \"https://theconversation.com/australian-businesses-have-actually-been-slow-to-adopt-ai-survey-finds-269812\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"news_media\",\n",
      "    \"platform_name\": \"The Conversation\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Windows Forum\",\n",
      "    \"url\": \"https://windowsforum.com/threads/australia-ai-adoption-from-copilot-wins-to-enterprise-scale-ai.389305/\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"unknown\",\n",
      "    \"platform_name\": \"Windows Forum\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Deloitte\",\n",
      "    \"url\": \"https://www.deloitte.com/au/en/services/consulting/analysis/state-generative-ai-enterprise.html\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"commercial\",\n",
      "    \"platform_name\": \"Deloitte\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"IT Brief Australia\",\n",
      "    \"url\": \"https://itbrief.com.au/story/australian-enterprises-lead-in-ai-adoption-but-face-data-hurdles\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"news_media\",\n",
      "    \"platform_name\": \"IT Brief Australia\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"PPC Land\",\n",
      "    \"url\": \"https://ppc.land/microsoft-ceo-admits-copilot-integrations-dont-really-work-as-adoption-falters/\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"news_media\",\n",
      "    \"platform_name\": \"PPC Land\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"The Missing Link\",\n",
      "    \"url\": \"https://www.themissinglink.com.au/news/navigating-copilot-adoption-challenges\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"news_media\",\n",
      "    \"platform_name\": \"The Missing Link\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Digital\",\n",
      "    \"url\": \"https://www.digital.gov.au/initiatives/copilot-trial/microsoft-365-copilot-evaluation-report-full/whole-government-adoption-generative-ai\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"government\",\n",
      "    \"platform_name\": \"Digital\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"CFO Dive\",\n",
      "    \"url\": \"https://www.cfodive.com/news/deloitte-ai-debacle-seen-wake-up-call-corporate-finance/802674/\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"news_media\",\n",
      "    \"platform_name\": \"CFO Dive\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Appinventiv\",\n",
      "    \"url\": \"https://appinventiv.com/blog/how-to-choose-right-ai-consulting-company-in-australia/\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"personal_blog\",\n",
      "    \"platform_name\": \"Appinventiv\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"PitchBook\",\n",
      "    \"url\": \"https://pitchbook.com/profiles/company/99585-55\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"commercial\",\n",
      "    \"platform_name\": \"PitchBook\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Mantel\",\n",
      "    \"url\": \"https://mantelgroup.com.au/\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"commercial\",\n",
      "    \"platform_name\": \"Mantel\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"BCG published \\\"AI Was Made for RevOps\\\" in September 2025\",\n",
      "    \"url\": null,\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"commercial\",\n",
      "    \"platform_name\": \"BCG\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": \"September 2025\",\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Nicola Lazzari\",\n",
      "    \"url\": \"https://nicolalazzari.ai/guides/ai-consultant-pricing-us\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"personal_blog\",\n",
      "    \"platform_name\": \"nicolalazzari.ai\",\n",
      "    \"stated_author\": \"Nicola Lazzari\",\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  },\n",
      "  {\n",
      "    \"raw_text\": \"Dasadvancedsystems\",\n",
      "    \"url\": \"https://dasadvancedsystems.com/blog/why-big-4-consulting-firms-are-failing-mid-size-companies-with-ai/\",\n",
      "    \"doi\": null,\n",
      "    \"arxiv_id\": null,\n",
      "    \"source_type\": \"personal_blog\",\n",
      "    \"platform_name\": \"Dasadvancedsystems.com\",\n",
      "    \"stated_author\": null,\n",
      "    \"stated_date\": null,\n",
      "    \"stated_metrics\": null\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Replace with your actual example\n",
    "test_input = AnalysisInput(\n",
    "    question=EXAMPLE_QUESTION,\n",
    "    answer=EXAMPLE_ANSWER,\n",
    "    context={\n",
    "        \"user_location\": \"AU\",\n",
    "        \"domain\": \"AI consulting\",  # e.g., \"healthcare\", \"finance\", \"technology\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Stage 1: Extract\n",
    "print(\"=== STAGE 1: EXTRACTION ===\")\n",
    "refs, extract_log = extract_references_v2(test_input)\n",
    "print(f\"Tokens: {extract_log.total_tokens} | Latency: {extract_log.latency_ms}ms\")\n",
    "print(f\"Found {len(refs)} references\")\n",
    "print(json.dumps(refs, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "17e1db55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 2: VERIFICATION ===\n",
      "\n",
      "Verifying 1/3: iTnews\n",
      "  Tokens: 1109 | Latency: 4178ms\n",
      "  Overall CRAAP Score: 3.6/5\n",
      "  Notes: The iTnews article seems generally relevant and authoritative, but the lack of a specific publicatio...\n",
      "\n",
      "Verifying 2/3: Substack\n",
      "  Tokens: 1124 | Latency: 4113ms\n",
      "  Overall CRAAP Score: 2.2/5\n",
      "  Notes: This Substack article discussing the AI threat to Salesforce is of limited value for researching the...\n",
      "\n",
      "Verifying 3/3: The Conversation\n",
      "  Tokens: 1018 | Latency: 3556ms\n",
      "  Overall CRAAP Score: 4.0/5\n",
      "  Notes: Key findings from search indicate that Australian businesses have been slow to adopt AI. This articl...\n",
      "\n",
      "=== FULL RESULTS ===\n",
      "[\n",
      "  {\n",
      "    \"reference\": {\n",
      "      \"raw_text\": \"iTnews\",\n",
      "      \"url\": \"https://www.itnews.com.au/feature/how-australian-businesses-can-overcome-the-struggle-to-move-ai-from-hype-to-reality-618510\",\n",
      "      \"doi\": null,\n",
      "      \"arxiv_id\": null,\n",
      "      \"source_type\": \"news_media\",\n",
      "      \"platform_name\": \"iTnews\",\n",
      "      \"stated_author\": null,\n",
      "      \"stated_date\": null,\n",
      "      \"stated_metrics\": null\n",
      "    },\n",
      "    \"verification\": {\n",
      "      \"url_accessible\": true,\n",
      "      \"currency\": {\n",
      "        \"score\": 3,\n",
      "        \"evidence\": \"The article doesn't have a clear publication date on the page itself, but the URL implies it was published in 2023 (618510). While this isn't extremely old, the AI field moves quickly, so more recent sources would be preferable. I will search for the publication date.\"\n",
      "      },\n",
      "      \"relevance\": {\n",
      "        \"score\": 4,\n",
      "        \"evidence\": \"The article discusses the challenges of implementing AI in Australian businesses, which is directly relevant to the user's context of AI consulting in Australia. It touches on the 'hype vs. reality' aspect, offering a potential angle for a unique selling proposition. The depth seems appropriate for a general business audience.\"\n",
      "      },\n",
      "      \"authority\": {\n",
      "        \"score\": 4,\n",
      "        \"evidence\": \"iTnews is a well-known Australian publication covering technology news. While the author isn't explicitly named in the provided information, iTnews generally has editorial oversight, suggesting a reasonable level of authority.\"\n",
      "      },\n",
      "      \"accuracy\": {\n",
      "        \"score\": 3,\n",
      "        \"evidence\": \"The article likely relies on reporting and industry insights rather than peer-reviewed research. It's difficult to verify the claims without more specific references or data within the article itself. Need to investigate if the article cites sources.\"\n",
      "      },\n",
      "      \"purpose\": {\n",
      "        \"score\": 4,\n",
      "        \"evidence\": \"The article appears to aim to inform readers about the challenges and opportunities of AI adoption in Australia. While iTnews might have a slight bias towards promoting technology adoption, there's no overt sales pitch or conflict of interest apparent from the title and context.\"\n",
      "      },\n",
      "      \"overall_score\": 3.6,\n",
      "      \"verification_notes\": \"The iTnews article seems generally relevant and authoritative, but the lack of a specific publication date and verifiable claims slightly weakens its overall score. It serves as a decent starting point for understanding the Australian AI market.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"reference\": {\n",
      "      \"raw_text\": \"Substack\",\n",
      "      \"url\": \"https://futureofprospecting.substack.com/p/the-ai-threat-that-could-break-salesforce\",\n",
      "      \"doi\": null,\n",
      "      \"arxiv_id\": null,\n",
      "      \"source_type\": \"personal_blog\",\n",
      "      \"platform_name\": \"Substack\",\n",
      "      \"stated_author\": null,\n",
      "      \"stated_date\": null,\n",
      "      \"stated_metrics\": null\n",
      "    },\n",
      "    \"verification\": {\n",
      "      \"url_accessible\": true,\n",
      "      \"currency\": {\n",
      "        \"score\": 2,\n",
      "        \"evidence\": \"The stated date is null, making it impossible to determine how current the information is. Given the rapidly evolving nature of AI and its impact on sales and consulting, the lack of a date is a significant drawback. It's possible there are more recent and relevant sources available.\"\n",
      "      },\n",
      "      \"relevance\": {\n",
      "        \"score\": 2,\n",
      "        \"evidence\": \"While the article discusses AI and its impact on Salesforce, it doesn't directly address the Sydney data and operation consulting/product markets. It has some tangential relevance to the AI consulting domain but lacks geographical specificity (Australia) and doesn't provide insights into unique selling propositions for the Sydney market. The depth is more general than the user's specific needs.\"\n",
      "      },\n",
      "      \"authority\": {\n",
      "        \"score\": 2,\n",
      "        \"evidence\": \"The author is not stated, making it difficult to assess their credentials or expertise. Substack is a platform that hosts various personal blogs; therefore, the authority depends heavily on the author, which is unknown in this case. There is no institutional backing.\"\n",
      "      },\n",
      "      \"accuracy\": {\n",
      "        \"score\": 2,\n",
      "        \"evidence\": \"It is difficult to assess the accuracy of the claims without knowing the author or their sources. The source doesn't appear to cite its own references. Given that it's a personal blog on Substack, it is unlikely to be peer-reviewed or professionally edited.\"\n",
      "      },\n",
      "      \"purpose\": {\n",
      "        \"score\": 3,\n",
      "        \"evidence\": \"The purpose seems to be to inform or perhaps persuade readers about the potential impact of AI on Salesforce. Without knowing the author's background, it's difficult to determine if there are any biases or conflicts of interest. It's unclear if it is trying to sell something directly.\"\n",
      "      },\n",
      "      \"overall_score\": 2.2,\n",
      "      \"verification_notes\": \"This Substack article discussing the AI threat to Salesforce is of limited value for researching the Sydney data and operation consulting/product markets due to the lack of a publication date, unknown author, and general focus. It lacks geographical relevance and specific insights needed to develop a unique selling proposition.\"\n",
      "    }\n",
      "  },\n",
      "  {\n",
      "    \"reference\": {\n",
      "      \"raw_text\": \"The Conversation\",\n",
      "      \"url\": \"https://theconversation.com/australian-businesses-have-actually-been-slow-to-adopt-ai-survey-finds-269812\",\n",
      "      \"doi\": null,\n",
      "      \"arxiv_id\": null,\n",
      "      \"source_type\": \"news_media\",\n",
      "      \"platform_name\": \"The Conversation\",\n",
      "      \"stated_author\": null,\n",
      "      \"stated_date\": null,\n",
      "      \"stated_metrics\": null\n",
      "    },\n",
      "    \"verification\": {\n",
      "      \"url_accessible\": true,\n",
      "      \"currency\": {\n",
      "        \"score\": 3,\n",
      "        \"evidence\": \"The article was published in 2024. While not the most recent, it is recent enough to provide insights into AI adoption in Australia. More recent sources might offer updated statistics, but this provides a good overview.\"\n",
      "      },\n",
      "      \"relevance\": {\n",
      "        \"score\": 4,\n",
      "        \"evidence\": \"The article directly discusses AI adoption by Australian businesses, making it geographically relevant. It provides a high-level overview, which is useful for understanding the market context for AI consulting.\"\n",
      "      },\n",
      "      \"authority\": {\n",
      "        \"score\": 4,\n",
      "        \"evidence\": \"The Conversation is a reputable platform that publishes articles by academics and experts. While the stated author is not named in the provided reference, The Conversation generally provides author information on the article page. The platform is known for its expertise and balanced reporting.\"\n",
      "      },\n",
      "      \"accuracy\": {\n",
      "        \"score\": 4,\n",
      "        \"evidence\": \"The Conversation typically cites sources and presents information based on research or expert opinions. Claims are generally verifiable, and the platform adheres to editorial standards.\"\n",
      "      },\n",
      "      \"purpose\": {\n",
      "        \"score\": 5,\n",
      "        \"evidence\": \"The primary purpose of the article is to inform. The Conversation aims to provide accessible and factual information on various topics, including technology and business. There's no apparent bias or intention to sell a specific product or service.\"\n",
      "      },\n",
      "      \"overall_score\": 4.0,\n",
      "      \"verification_notes\": \"Key findings from search indicate that Australian businesses have been slow to adopt AI. This article from The Conversation provides an overview of AI adoption in Australia.\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(\"=== STAGE 2: VERIFICATION ===\")\n",
    "\n",
    "verified_refs = []\n",
    "for i, ref in enumerate(refs[:3]):  # Limit to first 3 for testing\n",
    "    print(f\"\\nVerifying {i+1}/{min(3, len(refs))}: {ref.get('platform_name', 'unknown')}\")\n",
    "    \n",
    "    verification, verify_log = verify_reference_with_search(\n",
    "        ref, \n",
    "        test_input.question,\n",
    "        test_input.context\n",
    "    )\n",
    "    \n",
    "    print(f\"  Tokens: {verify_log.total_tokens} | Latency: {verify_log.latency_ms}ms\")\n",
    "    \n",
    "    if verification:\n",
    "        overall = verification.get('overall_score', 'N/A')\n",
    "        print(f\"  Overall CRAAP Score: {overall}/5\")\n",
    "        print(f\"  Notes: {verification.get('verification_notes', '')[:100]}...\")\n",
    "    \n",
    "    verified_refs.append({\n",
    "        \"reference\": ref,\n",
    "        \"verification\": verification\n",
    "    })\n",
    "\n",
    "print(\"\\n=== FULL RESULTS ===\")\n",
    "print(json.dumps(verified_refs, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db6302b",
   "metadata": {},
   "source": [
    "quick check of logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba2ee6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed numpy-2.4.0 pandas-2.3.3 pytz-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b490a6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>model</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>latency_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2026-01-08T01:04:51.496310</td>\n",
       "      <td>gemini-2.0-flash-exp</td>\n",
       "      <td>3746</td>\n",
       "      <td>1932</td>\n",
       "      <td>5678</td>\n",
       "      <td>10661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2026-01-08T01:05:39.939271</td>\n",
       "      <td>gemini-2.0-flash-exp</td>\n",
       "      <td>651</td>\n",
       "      <td>458</td>\n",
       "      <td>1109</td>\n",
       "      <td>4178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2026-01-08T01:05:44.053101</td>\n",
       "      <td>gemini-2.0-flash-exp</td>\n",
       "      <td>627</td>\n",
       "      <td>497</td>\n",
       "      <td>1124</td>\n",
       "      <td>4113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2026-01-08T01:05:47.609839</td>\n",
       "      <td>gemini-2.0-flash-exp</td>\n",
       "      <td>636</td>\n",
       "      <td>382</td>\n",
       "      <td>1018</td>\n",
       "      <td>3556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    timestamp                 model  prompt_tokens  \\\n",
       "0  2026-01-08T01:04:51.496310  gemini-2.0-flash-exp           3746   \n",
       "1  2026-01-08T01:05:39.939271  gemini-2.0-flash-exp            651   \n",
       "2  2026-01-08T01:05:44.053101  gemini-2.0-flash-exp            627   \n",
       "3  2026-01-08T01:05:47.609839  gemini-2.0-flash-exp            636   \n",
       "\n",
       "   completion_tokens  total_tokens  latency_ms  \n",
       "0               1932          5678       10661  \n",
       "1                458          1109        4178  \n",
       "2                497          1124        4113  \n",
       "3                382          1018        3556  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_logs = pd.DataFrame([log.to_dict() for log in llm_logs])\n",
    "df_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d1a227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19ae04db",
   "metadata": {},
   "source": [
    "# iteration 3\n",
    "Include more logging, aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da6d00c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ generate_with_logging defined\n",
      "✅ Setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from google import genai\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "\n",
    "@dataclass\n",
    "class LLMLog:\n",
    "    timestamp: str\n",
    "    model: str\n",
    "    task: str\n",
    "    prompt_tokens: int\n",
    "    completion_tokens: int\n",
    "    total_tokens: int\n",
    "    latency_ms: int\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "# Store logs for this session\n",
    "llm_logs: list[LLMLog] = []\n",
    "\n",
    "def generate_with_logging(prompt: str, task: str, model: str = \"gemini-2.0-flash-exp\") -> tuple[str, LLMLog]:\n",
    "    \"\"\"Generate content and log token usage + latency.\"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    latency_ms = int((time.time() - start_time) * 1000)\n",
    "    \n",
    "    usage = response.usage_metadata\n",
    "    log = LLMLog(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model=model,\n",
    "        task=task,\n",
    "        prompt_tokens=usage.prompt_token_count,\n",
    "        completion_tokens=usage.candidates_token_count,\n",
    "        total_tokens=usage.total_token_count,\n",
    "        latency_ms=latency_ms\n",
    "    )\n",
    "    \n",
    "    llm_logs.append(log)\n",
    "    \n",
    "    return response.text, log\n",
    "\n",
    "print(\"✅ generate_with_logging defined\")\n",
    "\n",
    "print(\"✅ Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278e1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('example_answer.txt',mode='r') as file:\n",
    "    EXAMPLE_ANSWER = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc08551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Data structures defined\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "class SourceType(str, Enum):\n",
    "    ACADEMIC_JOURNAL = \"academic_journal\"\n",
    "    ACADEMIC_PREPRINT = \"academic_preprint\"\n",
    "    NEWS_MEDIA = \"news_media\"\n",
    "    GOVERNMENT = \"government\"\n",
    "    SOCIAL_MEDIA = \"social_media\"\n",
    "    PERSONAL_BLOG = \"personal_blog\"\n",
    "    DOCUMENTATION = \"documentation\"\n",
    "    COMMERCIAL = \"commercial\"\n",
    "    UNKNOWN = \"unknown\"\n",
    "\n",
    "@dataclass\n",
    "class ExtractedReference:\n",
    "    \"\"\"Stage 1 output - extracted from text only, no inference.\"\"\"\n",
    "    raw_text: str\n",
    "    url: Optional[str] = None\n",
    "    doi: Optional[str] = None\n",
    "    arxiv_id: Optional[str] = None\n",
    "    source_type: SourceType = SourceType.UNKNOWN\n",
    "    platform_name: Optional[str] = None\n",
    "    # Only what's explicitly in text\n",
    "    stated_author: Optional[str] = None\n",
    "    stated_date: Optional[str] = None\n",
    "    stated_metrics: Optional[str] = None  # e.g., \"2,400 upvotes\" as string\n",
    "\n",
    "@dataclass \n",
    "class CRAAPScore:\n",
    "    \"\"\"Stage 2 output - requires verification.\"\"\"\n",
    "    score: int  # 1-5\n",
    "    evidence: str  # Why this score\n",
    "    \n",
    "@dataclass\n",
    "class VerifiedReference:\n",
    "    \"\"\"Stage 2 output - after search/verification.\"\"\"\n",
    "    reference: ExtractedReference\n",
    "    url_accessible: Optional[bool] = None\n",
    "    currency: Optional[CRAAPScore] = None\n",
    "    relevance: Optional[CRAAPScore] = None\n",
    "    authority: Optional[CRAAPScore] = None\n",
    "    accuracy: Optional[CRAAPScore] = None\n",
    "    purpose: Optional[CRAAPScore] = None\n",
    "    overall_score: Optional[float] = None\n",
    "    verification_notes: str = \"\"\n",
    "\n",
    "\n",
    "print(\"✅ Data structures defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f6f033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ extract_references defined\n"
     ]
    }
   ],
   "source": [
    "EXTRACTION_PROMPT_V3 = \"\"\"\n",
    "You are a reference extraction system. Extract ALL references from this AI-generated text.\n",
    "\n",
    "Only extract information EXPLICITLY stated. Use null for anything not directly written.\n",
    "\n",
    "For each reference provide:\n",
    "- raw_text: Exact citation as written\n",
    "- url: URL if present, null otherwise\n",
    "- doi: DOI if present, null otherwise  \n",
    "- arxiv_id: arXiv ID if present, null otherwise\n",
    "- source_type: One of: academic_journal, academic_preprint, news_media, government, social_media, personal_blog, documentation, commercial, unknown\n",
    "- platform_name: Specific platform/publication name\n",
    "- stated_author: Author name if explicitly written, null otherwise\n",
    "- stated_date: Publication date/year if written, null otherwise\n",
    "- stated_metrics: Any engagement metrics as written (e.g., \"50k views\"), null otherwise\n",
    "\n",
    "TEXT TO ANALYZE:\n",
    "---\n",
    "{answer}\n",
    "---\n",
    "\n",
    "Return ONLY a JSON array. Do not hallucinate - null for anything not explicit.\n",
    "\"\"\"\n",
    "\n",
    "def extract_references(answer: str) -> tuple[list[dict], LLMLog]:\n",
    "    \"\"\"Extract references from answer text.\"\"\"\n",
    "    \n",
    "    prompt = EXTRACTION_PROMPT_V3.format(answer=answer)\n",
    "    response_text, log = generate_with_logging(prompt,task='extract_references')\n",
    "    \n",
    "    try:\n",
    "        clean_text = response_text.strip()\n",
    "        if clean_text.startswith(\"```json\"):\n",
    "            clean_text = clean_text[7:]\n",
    "        if clean_text.startswith(\"```\"):\n",
    "            clean_text = clean_text[3:]\n",
    "        if clean_text.endswith(\"```\"):\n",
    "            clean_text = clean_text[:-3]\n",
    "        \n",
    "        return json.loads(clean_text.strip()), log\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: {e}\\nRaw: {response_text}\")\n",
    "        return [], log\n",
    "\n",
    "print(\"✅ extract_references defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0929621d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ verify_reference defined (with logging)\n"
     ]
    }
   ],
   "source": [
    "from google.genai import types\n",
    "\n",
    "CRAAP_PROMPT_V2 = \"\"\"\n",
    "You are a source verification system. Evaluate this reference using web search.\n",
    "\n",
    "REFERENCE:\n",
    "{reference_json}\n",
    "\n",
    "Search for this source and evaluate on CRAAP:\n",
    "\n",
    "**Currency (1-5):** How recent? Still relevant?\n",
    "**Relevance (1-5):** Is this a substantive source for the topic?\n",
    "**Authority (1-5):** Who wrote it? Are they credible?\n",
    "**Accuracy (1-5):** Can claims be verified? Does it cite sources?\n",
    "**Purpose (1-5):** Inform, persuade, or sell?\n",
    "\n",
    "Also check: Can the URL be accessed?\n",
    "\n",
    "Return JSON only:\n",
    "{{\n",
    "  \"url_accessible\": true/false/null,\n",
    "  \"currency\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"relevance\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"authority\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"accuracy\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"purpose\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"overall_score\": 1.0-5.0,\n",
    "  \"summary\": \"One sentence summary\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def verify_reference(reference: dict, model: str = \"gemini-2.0-flash-exp\") -> tuple[dict, LLMLog]:\n",
    "    \"\"\"Verify a single reference using search. Returns result and log.\"\"\"\n",
    "    \n",
    "    prompt = CRAAP_PROMPT_V2.format(reference_json=json.dumps(reference, indent=2))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=[types.Tool(google_search=types.GoogleSearch())]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    latency_ms = int((time.time() - start_time) * 1000)\n",
    "    \n",
    "    usage = response.usage_metadata\n",
    "    log = LLMLog(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model=model,\n",
    "        task='verify_reference',\n",
    "        prompt_tokens=usage.prompt_token_count,\n",
    "        completion_tokens=usage.candidates_token_count,\n",
    "        total_tokens=usage.total_token_count,\n",
    "        latency_ms=latency_ms\n",
    "    )\n",
    "    llm_logs.append(log)  # Now logging!\n",
    "    \n",
    "    try:\n",
    "        clean_text = response.text.strip()\n",
    "        if clean_text.startswith(\"```json\"):\n",
    "            clean_text = clean_text[7:]\n",
    "        if clean_text.startswith(\"```\"):\n",
    "            clean_text = clean_text[3:]\n",
    "        if clean_text.endswith(\"```\"):\n",
    "            clean_text = clean_text[:-3]\n",
    "        \n",
    "        return json.loads(clean_text.strip()), log\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: {e}\\nRaw: {response.text}\")\n",
    "        return {}, log\n",
    "\n",
    "print(\"✅ verify_reference defined (with logging)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a12df8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 1: EXTRACTION ===\n",
      "Tokens: 5483 | Latency: 11000ms\n",
      "Found 15 references\n",
      "\n",
      "=== STAGE 2: VERIFICATION ===\n",
      "[1/15] iTnews\n",
      "       CRAAP: 4.4/5 | 5957ms\n",
      "[2/15] Substack\n",
      "       CRAAP: 4.0/5 | 4718ms\n",
      "[3/15] The Conversation\n",
      "       CRAAP: 4.6/5 | 4047ms\n",
      "[4/15] Windows Forum\n",
      "       CRAAP: 3.0/5 | 4997ms\n",
      "[5/15] Deloitte\n",
      "       CRAAP: 4.4/5 | 6557ms\n",
      "[6/15] IT Brief Australia\n",
      "       CRAAP: 4.4/5 | 6951ms\n",
      "[7/15] PPC Land\n",
      "       CRAAP: 4.4/5 | 6779ms\n",
      "[8/15] The Missing Link\n",
      "       CRAAP: 4.2/5 | 4922ms\n",
      "[9/15] Digital\n",
      "       CRAAP: 4.4/5 | 5889ms\n",
      "[10/15] CFO Dive\n",
      "       CRAAP: 4.6/5 | 6339ms\n",
      "[11/15] Appinventiv\n",
      "       CRAAP: 4.0/5 | 5825ms\n",
      "[12/15] PitchBook\n",
      "       CRAAP: 4.4/5 | 5419ms\n",
      "[13/15] Mantel\n",
      "       CRAAP: 4.6/5 | 5016ms\n",
      "[14/15] Nicola Lazzari\n",
      "       CRAAP: 4.4/5 | 5194ms\n",
      "[15/15] Dasadvancedsystems\n",
      "       CRAAP: 4.2/5 | 5191ms\n",
      "\n",
      "✅ Verification complete\n"
     ]
    }
   ],
   "source": [
    "# Stage 1: Extract\n",
    "print(\"=== STAGE 1: EXTRACTION ===\")\n",
    "refs, extract_log = extract_references(EXAMPLE_ANSWER)\n",
    "print(f\"Tokens: {extract_log.total_tokens} | Latency: {extract_log.latency_ms}ms\")\n",
    "print(f\"Found {len(refs)} references\\n\")\n",
    "\n",
    "# Stage 2: Verify each\n",
    "print(\"=== STAGE 2: VERIFICATION ===\")\n",
    "verified = []\n",
    "for i, ref in enumerate(refs):\n",
    "    print(f\"[{i+1}/{len(refs)}] {ref.get('platform_name', ref.get('url', 'unknown')[:30])}\")\n",
    "    result, log = verify_reference(ref)\n",
    "    if result:\n",
    "        print(f\"       CRAAP: {result.get('overall_score', 'N/A')}/5 | {log.latency_ms}ms\")\n",
    "    verified.append({\"reference\": ref, \"craap\": result})\n",
    "\n",
    "print(\"\\n✅ Verification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d9e8162",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'reference': {'raw_text': 'iTnews',\n",
       "   'url': 'https://www.itnews.com.au/feature/how-australian-businesses-can-overcome-the-struggle-to-move-ai-from-hype-to-reality-618510',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'news_media',\n",
       "   'platform_name': 'iTnews',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article was published on 2025-07-09, making it recent and relevant in January 2026.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article directly addresses the challenges Australian businesses face in implementing AI, which is a substantive topic.'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'iTnews is a well-established Australian technology news publication. They have won awards for technology journalism and are considered a reliable source for IT professionals. Ry Crozier is the Editor in Chief. Velvet-Belle Templeman is the Publishing Director.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': \"The article presents data and insights regarding AI implementation challenges. While specific sources within the article aren't explicitly cited with hyperlinks, the claims made align with the general understanding of AI adoption hurdles. iTnews has an editorial policy that emphasizes accuracy and transparency [1].\"},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': 'The primary purpose appears to be to inform readers about the challenges and potential solutions for AI implementation in Australian businesses. While iTnews does accept sponsored content, this particular article does not appear to be promotional.'},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': 'This iTnews article discusses the challenges Australian businesses face when trying to move AI from hype to reality, offering insights and potential solutions like the micro-innovation approach.'}},\n",
       " {'reference': {'raw_text': 'Substack',\n",
       "   'url': 'https://futureofprospecting.substack.com/p/the-ai-threat-that-could-break-salesforce',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'social_media',\n",
       "   'platform_name': 'Substack',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article was published on Substack on September 9, 2025, making it very recent and relevant for current analysis in 2026.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article directly addresses the impact of AI on Salesforce and HubSpot, which is a substantive topic for understanding technology trends in sales and CRM.'},\n",
       "   'authority': {'score': 3,\n",
       "    'evidence': \"The author is 'Revenoid' on Substack's 'Future of Prospecting'. While the author's specific credentials aren't immediately detailed, the Substack focuses on AI in sales, suggesting expertise. Further investigation into Revenoid's background would be needed for a higher authority score.\"},\n",
       "   'accuracy': {'score': 3,\n",
       "    'evidence': \"The article makes claims about Salesforce's revenue model and AI adoption, which are verifiable. It mentions Salesforce cutting support jobs due to AI, which can be cross-referenced. However, the article doesn't have extensive citations, so the accuracy relies on the reader's ability to verify claims independently.\"},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': \"The article primarily aims to inform about the potential disruption AI could cause to established CRM systems like Salesforce and HubSpot. While it also subtly promotes 'Revenoid' and related concepts, the main focus is on analyzing industry trends.\"},\n",
       "   'overall_score': 4.0,\n",
       "   'summary': \"The Substack article 'The AI Threat That Could Break Salesforce & HubSpot' discusses how AI adoption could negatively impact Salesforce's seat-based revenue model, offering analysis on the evolving landscape of AI in sales.\"}},\n",
       " {'reference': {'raw_text': 'The Conversation',\n",
       "   'url': 'https://theconversation.com/australian-businesses-have-actually-been-slow-to-adopt-ai-survey-finds-269812',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'news_media',\n",
       "   'platform_name': 'The Conversation',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article was published on November 17, 2025, making it very recent.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article directly addresses the adoption of AI in Australian businesses, which is a substantive topic.'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'The article is written by Stan Karanasios, a Professor in Information Systems at The University of Queensland. The Conversation is also a reputable source. The author declares no conflicts of interest.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': 'The article cites a Reserve Bank of Australia survey and aligns with other reports. It presents findings and observations, enhancing its verifiability.'},\n",
       "   'purpose': {'score': 5,\n",
       "    'evidence': 'The article aims to inform about the current state of AI adoption in Australian businesses based on a recent survey. It is an informative piece without an apparent persuasive or sales agenda.'},\n",
       "   'overall_score': 4.6,\n",
       "   'summary': 'This article from The Conversation, published in November 2025, discusses the slow adoption of AI by Australian businesses based on a survey by the Reserve Bank of Australia, highlighting reasons and comparisons to other countries.'}},\n",
       " {'reference': {'raw_text': 'Windows Forum',\n",
       "   'url': 'https://windowsforum.com/threads/australia-ai-adoption-from-copilot-wins-to-enterprise-scale-ai.389305/',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'social_media',\n",
       "   'platform_name': 'Windows Forum',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The forum post was created in 2024, and the discussion is ongoing, making it relatively recent and still relevant as of January 8, 2026.'},\n",
       "   'relevance': {'score': 3,\n",
       "    'evidence': 'The forum discusses AI adoption in Australia, which is relevant to the topic, but the specific focus on Copilot might narrow its broader applicability to enterprise-scale AI.'},\n",
       "   'authority': {'score': 2,\n",
       "    'evidence': 'The source is a forum, so the author is not a recognized expert. Credibility is limited.'},\n",
       "   'accuracy': {'score': 2,\n",
       "    'evidence': 'Claims on a forum are generally difficult to verify and often lack citations. Accuracy depends on the individual posters.'},\n",
       "   'purpose': {'score': 3,\n",
       "    'evidence': 'The purpose is primarily to inform and discuss, but individual posters may have biases or promotional intentions.'},\n",
       "   'overall_score': 3.0,\n",
       "   'summary': 'The Windows Forum thread discusses AI adoption in Australia with a focus on Microsoft Copilot, but its lack of authoritativeness and verifiable claims lowers its overall score as a reliable source.'}},\n",
       " {'reference': {'raw_text': 'Deloitte',\n",
       "   'url': 'https://www.deloitte.com/au/en/services/consulting/analysis/state-generative-ai-enterprise.html',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'commercial',\n",
       "   'platform_name': 'Deloitte',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 4,\n",
       "    'evidence': 'The source discusses Deloitte\\'s \"State of Generative AI in the Enterprise\" report. Search results show that Deloitte has been actively publishing reports on this topic throughout 2024 [2, 5, 6, 9]. Also, Deloitte insights were published throughout 2025 [7, 12]. Deloitte also posted a video in December of 2024 related to Strategy and Operations [14]. TMT Predictions for 2026 mentions AI [11].'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': \"The source is directly relevant as it discusses Deloitte's work and insights on Generative AI in the enterprise, which is a current and important topic for businesses.\"},\n",
       "   'authority': {'score': 5,\n",
       "    'evidence': 'Deloitte is a well-known and respected global professional services network [10]. They have a dedicated AI Institute and Generative AI practice [3, 4]. This establishes their authority in the field of AI and business consulting.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': \"Deloitte's reports are based on surveys and analysis of data from numerous leaders and organizations [2, 6]. Deloitte also provides industry-leading audit, consulting, tax and advisory services to many of the world's most admired brands, including nearly 90% of the Fortune 500® and more than 7,000 private companies [4]. While the claims are likely accurate given Deloitte's reputation, the source doesn't provide detailed citations or methodologies within the document itself, requiring further investigation to verify specific claims.\"},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': \"While the primary purpose appears to be to inform about Generative AI trends and Deloitte's capabilities in this area, there's also an element of promoting Deloitte's services and expertise to potential clients [3, 4].\"},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': 'The Deloitte source appears to be a relevant and authoritative source on the state of Generative AI in the enterprise, providing insights based on their research and consulting work; it is important to note that the source has a promotional element as well.'}},\n",
       " {'reference': {'raw_text': 'IT Brief Australia',\n",
       "   'url': 'https://itbrief.com.au/story/australian-enterprises-lead-in-ai-adoption-but-face-data-hurdles',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'news_media',\n",
       "   'platform_name': 'IT Brief Australia',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The website is active and contains articles published in the last few days, including today, January 8, 2026. For instance, an article titled \"Healthcare now operates inside the audit\" was published on January 7, 2026 [8].'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article \"Australian enterprises lead in AI adoption but face data hurdles\" directly addresses the topic of AI adoption in Australian enterprises [1]. The website IT Brief Australia focuses on technology news for CIOs and IT decision-makers in Australia, making it a relevant source for technology and business-related topics [2].'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'IT Brief Australia is a technology news website targeting CIOs and IT managers in large Australian enterprises and government organizations [2]. The site features expert columns from individuals in reputable positions, such as CTOs and Managing Directors from companies like Nutanix APAC and Qualys [2]. This suggests the content is reviewed or provided by industry professionals.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': 'The article about AI adoption references a global survey conducted by OpenText in partnership with the Ponemon Institute, which lends credibility to its claims [1]. Other articles cite research from reputable sources like the University of Technology Sydney [9] and Gartner [6]. However, not all claims are explicitly sourced, so while generally accurate, some information may require further verification.'},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': 'IT Brief Australia primarily aims to inform readers about technology trends, industry news, and insights relevant to IT professionals [2]. While the site includes sponsored content and media kits [8, 9, 12], its main purpose is to provide news and analysis rather than overt persuasion or direct sales.'},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': 'IT Brief Australia is a relevant and current news source for technology professionals in Australia, providing news and insights on topics such as AI adoption, cybersecurity, and digital transformation; it is generally credible and accurate, though some claims may benefit from further verification.'}},\n",
       " {'reference': {'raw_text': 'PPC Land',\n",
       "   'url': 'https://ppc.land/microsoft-ceo-admits-copilot-integrations-dont-really-work-as-adoption-falters/',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'news_media',\n",
       "   'platform_name': 'PPC Land',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article is dated December 29, 2025, making it very recent and relevant to current events.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': \"The article directly discusses Microsoft's Copilot and its integration issues, which is a substantive topic in the tech and AI field.\"},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'PPC Land is owned and operated by Luis Rijo, a marketing professional with over a decade of experience (Source: PPC Land About page). They are also listed as a top 100 tech publication on the Ghost ecosystem. They also link to sources to increase transparency.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': 'The article references a report from The Information and includes direct quotes attributed to Microsoft CEO Satya Nadella, suggesting reliance on credible sources. PPC Land also states they link to sources to increase transparency.'},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': 'PPC Land seems to primarily inform its readers about digital marketing and AI news, though it does contain advertising.'},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': \"PPC Land published an article on December 29, 2025, reporting that Microsoft CEO Satya Nadella criticized Copilot's Gmail and Outlook integrations, stating they 'don't really work', based on a report from The Information.\"}},\n",
       " {'reference': {'raw_text': 'The Missing Link',\n",
       "   'url': 'https://www.themissinglink.com.au/news/navigating-copilot-adoption-challenges',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'news_media',\n",
       "   'platform_name': 'The Missing Link',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article was published on 2025-08-06, making it very recent and relevant in the rapidly evolving field of AI.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article directly addresses the challenges of Copilot adoption, which is the specified topic.'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'The Missing Link is an established IT solutions and services provider (25+ years). They partner with industry-leading experts. Matt Dunn, Head of Automation & AI at The Missing Link is an experienced trainer with a strong foundation in management consulting and technology.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': \"The article cites Gartner's research to support its claims about Copilot adoption rates and licensing costs. The article provides actionable insights and recommendations that align with common challenges discussed in other sources.\"},\n",
       "   'purpose': {'score': 3,\n",
       "    'evidence': \"The article primarily informs readers about the challenges of Copilot adoption. While it also promotes The Missing Link's services in building adoption plans, the informative aspect is more pronounced.\"},\n",
       "   'overall_score': 4.2,\n",
       "   'summary': \"The Missing Link's article discusses challenges in Microsoft Copilot adoption, including the need for training, data management, and strategic planning, highlighting the company's expertise in providing tailored adoption plans.\"}},\n",
       " {'reference': {'raw_text': 'Digital',\n",
       "   'url': 'https://www.digital.gov.au/initiatives/copilot-trial/microsoft-365-copilot-evaluation-report-full/whole-government-adoption-generative-ai',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'government',\n",
       "   'platform_name': 'Digital',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 4,\n",
       "    'evidence': \"The report discusses the Australian Government's trial of Microsoft 365 Copilot. While the 'stated_date' is missing, other sources ([2], [5]) indicate the trial occurred roughly between January and June 2024, and the report was published around that time. Given the current date is January 8, 2026, the information is still relatively recent and relevant.\"},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The report is a direct evaluation of Microsoft 365 Copilot within the Australian government, making it highly relevant for understanding the adoption and impact of generative AI in a government context.'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'The report is published on digital.gov.au, a website of the Australian Digital Transformation Agency (DTA). The DTA is the Australian government agency responsible for leading digital transformation, lending credibility to the report. However, a stated author is missing.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': 'The report presents findings from a government trial, including supporting data, tables, and graphs ([1]). Other sources ([3], [5]) reference the report and its findings, suggesting that the claims can be verified. However, a lack of explicit citations within the document makes it difficult to fully assess accuracy.'},\n",
       "   'purpose': {'score': 5,\n",
       "    'evidence': \"The report aims to inform and provide a detailed analysis of the evaluation findings from the Australian Government's trial of Microsoft 365 Copilot, focusing on employee-related outcomes, productivity, and the broader adoption of generative AI. This suggests an informative purpose.\"},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': \"The Microsoft 365 Copilot Evaluation Report on digital.gov.au is a relevant and informative document detailing the Australian government's trial of Copilot, providing insights into its impact and adoption, although the lack of a stated author and explicit citations slightly impacts its authority and accuracy scores.\"}},\n",
       " {'reference': {'raw_text': 'CFO Dive',\n",
       "   'url': 'https://www.cfodive.com/news/deloitte-ai-debacle-seen-wake-up-call-corporate-finance/802674/',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'news_media',\n",
       "   'platform_name': 'CFO Dive',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article was published on 2025-10-14, making it very recent and relevant in January 2026. [2]'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': \"The article directly addresses the 'Deloitte AI debacle' which is a significant event in corporate finance, making it highly relevant to the topic. [2]\"},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'CFO Dive is a publication of Industry Dive, now a subsidiary of Informa TechTarget, a reputable source for financial news. [1, 5] Its editorial team provides in-depth journalism on impactful news and trends. [4]'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': \"CFO Dive is rated high for factual reporting due to proper sourcing and a clean fact-check record. [1] The article in question discusses a specific event (Deloitte's AI errors) that can be verified through other news sources. [2, 12, 13]\"},\n",
       "   'purpose': {'score': 5,\n",
       "    'evidence': 'The primary purpose of CFO Dive is to inform readers about news and trends in corporate finance. [5, 6] While they accept opinion pieces, the article in question appears to be a news report on a specific event. [3, 7]'},\n",
       "   'overall_score': 4.6,\n",
       "   'summary': \"CFO Dive is a reputable news source that published an article on October 14, 2025, regarding the 'Deloitte AI debacle,' which serves as a wake-up call for corporate finance, highlighting the risks of using AI tools without proper safeguards.\"}},\n",
       " {'reference': {'raw_text': 'Appinventiv',\n",
       "   'url': 'https://appinventiv.com/blog/how-to-choose-right-ai-consulting-company-in-australia/',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'personal_blog',\n",
       "   'platform_name': 'Appinventiv',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article \"How to Choose an AI Consulting Partner in Australia\" was published on November 28, 2025 [2], making it very recent and relevant for current assessment.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article directly addresses the topic of choosing an AI consulting company in Australia, offering specific advice and mentioning some relevant companies [2].'},\n",
       "   'authority': {'score': 3,\n",
       "    'evidence': 'Appinventiv is a registered company [6, 11] that offers AI and app development services [3, 5, 6, 8, 9]. They claim expertise and experience in the field. However, some reviews mention communication and code quality issues [1].'},\n",
       "   'accuracy': {'score': 3,\n",
       "    'evidence': \"While the article provides advice based on Appinventiv's perspective, the claims are not explicitly backed by external sources or citations within the article itself [2]. Some reviews suggest potential issues with their services [1, 3].\"},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': \"The primary purpose is likely to inform readers about choosing an AI consulting company, but it also subtly promotes Appinventiv's services as a potential solution [2]. The blog post format suggests it serves a marketing purpose as well.\"},\n",
       "   'overall_score': 4.0,\n",
       "   'summary': 'The Appinventiv blog post provides recent advice on selecting an AI consulting partner in Australia, but while Appinventiv demonstrates expertise, some reviews raise concerns about communication and code quality, and the article serves a marketing purpose.'}},\n",
       " {'reference': {'raw_text': 'PitchBook',\n",
       "   'url': 'https://pitchbook.com/profiles/company/99585-55',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'commercial',\n",
       "   'platform_name': 'PitchBook',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'PitchBook provides real-time data and is widely used in the financial industry. Many reviews and articles are dated in 2025 and 2026, indicating current relevance. [2, 4, 6]'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': \"PitchBook is a well-known platform for private market intelligence, offering deep insights into companies, investors, and financial transactions. It's a valuable tool for professionals in venture capital, private equity, M&A, and even founders raising capital. [1]\"},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'PitchBook is a reputable financial data and software company acquired by Morningstar in 2016. It has a large team, global offices, and serves thousands of professionals. [3, 2, 10]'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': 'PitchBook aggregates financial data from thousands of sources and is known for providing high-quality, well-organized data. However, some reviews mention occasional data gaps or inaccuracies. Claims can generally be verified through cross-referencing with other financial data sources. [1, 4, 6]'},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': 'PitchBook primarily aims to inform and provide data and analytics to professionals in the private capital markets. While it is a commercial platform requiring a subscription, its main purpose is to deliver market intelligence rather than solely to persuade or sell. [2, 1]'},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': 'PitchBook is a reputable and relevant financial data platform offering comprehensive insights into the private capital markets, although some users have noted occasional data inaccuracies.'}},\n",
       " {'reference': {'raw_text': 'Mantel',\n",
       "   'url': 'https://mantelgroup.com.au/',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'commercial',\n",
       "   'platform_name': 'Mantel',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The website ([https://mantelgroup.com.au/](https://mantelgroup.com.au/)) mentions awards and partnerships for 2024 and 2025, indicating recent activity and relevance. The InnovAGEING Hub is holding an ITAC Conference in 2026 [2].'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': \"The source is Mantel Group's website, so it's directly relevant for providing information about the company, its services (data, digital, cyber security, and cloud expertise), and its values.\"},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'Mantel Group is a technology consulting company operating in Australia and New Zealand. They are recognized as partners with major platforms like AWS, Google Cloud, and Microsoft Azure [5]. They have also received awards [1, 5].'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': \"The claims made on the website about their services and partnerships can likely be verified through external sources like AWS, Google Cloud, and Microsoft. They are also recognized as a 'Great Place to Work' [4, 5].\"},\n",
       "   'purpose': {'score': 5,\n",
       "    'evidence': \"The website primarily serves to promote Mantel Group's services and attract potential clients and employees. It highlights their expertise, awards, and company culture.\"},\n",
       "   'overall_score': 4.6,\n",
       "   'summary': \"Mantel Group's website is a current, relevant, and authoritative source of information about the company, its services, and its values, primarily serving promotional purposes with claims that appear to be verifiable.\"}},\n",
       " {'reference': {'raw_text': 'Nicola Lazzari',\n",
       "   'url': 'https://nicolalazzari.ai/guides/ai-consultant-pricing-us',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'personal_blog',\n",
       "   'platform_name': 'Nicola Lazzari',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'Many pages on the site are dated 2025, and some mention data for 2025-2027, so the site is current as of 2026-01-08.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The blog focuses on AI consulting, pricing, and related topics, which makes it a substantive source if the user is interested in that topic.'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': 'The author, Nicola Lazzari, is an AI consultant and creative technologist. The site details his experience and services offered, lending credibility.'},\n",
       "   'accuracy': {'score': 4,\n",
       "    'evidence': 'Claims regarding pricing and services seem verifiable, and the site provides detailed explanations. The site also uses AI to answer questions about its content, which are cited.'},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': \"The site primarily aims to inform and market Nicola Lazzari's AI consulting services. While informative, there is an underlying persuasive element to attract potential clients.\"},\n",
       "   'overall_score': 4.4,\n",
       "   'summary': 'This is the personal blog for Nicola Lazzari, an AI consultant, which seems to be current, relevant, and fairly accurate, with the primary purpose of informing and promoting his services.'}},\n",
       " {'reference': {'raw_text': 'Dasadvancedsystems',\n",
       "   'url': 'https://dasadvancedsystems.com/blog/why-big-4-consulting-firms-are-failing-mid-size-companies-with-ai/',\n",
       "   'doi': None,\n",
       "   'arxiv_id': None,\n",
       "   'source_type': 'personal_blog',\n",
       "   'platform_name': 'Dasadvancedsystems',\n",
       "   'stated_author': None,\n",
       "   'stated_date': None,\n",
       "   'stated_metrics': None},\n",
       "  'craap': {'url_accessible': True,\n",
       "   'currency': {'score': 5,\n",
       "    'evidence': 'The article \"Why Big 4 Consulting Firms are Failing Mid-Size Companies with AI\" was published on August 26, 2025, which is relatively recent.'},\n",
       "   'relevance': {'score': 5,\n",
       "    'evidence': 'The article directly addresses the topic of AI consulting and the challenges faced by mid-size companies, making it highly relevant.'},\n",
       "   'authority': {'score': 4,\n",
       "    'evidence': \"The article is published on Das Advanced Systems' blog. Abhinav Das is mentioned as the author, and the website focuses on AI consulting for mid-sized companies, suggesting some expertise. However, more information about Abhinav Das's credentials would strengthen this assessment.\"},\n",
       "   'accuracy': {'score': 3,\n",
       "    'evidence': \"The article presents a clear argument and provides some specific examples. However, it's difficult to verify the claims without external sources or citations. The article does not cite external sources.\"},\n",
       "   'purpose': {'score': 4,\n",
       "    'evidence': 'The article primarily aims to inform readers about the challenges mid-size companies face with Big 4 consulting firms and subtly promotes Das Advanced Systems as an alternative. There are elements of persuasion as it attempts to convince readers of the shortcomings of larger firms.'},\n",
       "   'overall_score': 4.2,\n",
       "   'summary': 'The blog post on Dasadvancedsystems.com, published in August 2025, discusses why Big 4 consulting firms may not be the best fit for mid-size companies seeking AI solutions, suggesting that specialized firms like Das Advanced Systems might be a better alternative; the article is relatively recent and relevant but lacks external citations.'}}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d862c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ verify_reference defined (strict mode)\n"
     ]
    }
   ],
   "source": [
    "# let's try again with a stricter source checker.\n",
    "\n",
    "CRAAP_PROMPT_STRICT = \"\"\"\n",
    "You are a SKEPTICAL source verification system. Your job is to be critical and identify weak sources.\n",
    "\n",
    "REFERENCE:\n",
    "{reference_json}\n",
    "\n",
    "Search for this source and evaluate STRICTLY on CRAAP. Be harsh - most sources are mediocre.\n",
    "\n",
    "SCORING GUIDE (err on the lower side):\n",
    "\n",
    "**Currency (1-5):**\n",
    "- 5: Published within last year, regularly updated\n",
    "- 3: 1-3 years old, still somewhat relevant  \n",
    "- 1: Outdated, stale, or no date available\n",
    "\n",
    "**Relevance (1-5):**\n",
    "- 5: Primary source, directly addresses topic with depth\n",
    "- 3: Tangentially related, surface-level coverage\n",
    "- 1: Barely relevant, clickbait title, or off-topic\n",
    "\n",
    "**Authority (1-5):**\n",
    "- 5: Named expert with verifiable credentials, institutional backing, peer-reviewed\n",
    "- 3: Professional journalist at known outlet, or practitioner with some track record\n",
    "- 2: Anonymous or pseudonymous author, no credentials stated\n",
    "- 1: Random blog, no author info, self-published with no reputation\n",
    "\n",
    "IMPORTANT: Personal blogs, Substack, Medium, etc. should START at 2 and only go higher if the author has VERIFIABLE expertise (real name, credentials, institutional affiliation). \"Thought leaders\" and \"consultants\" without specific credentials = 2.\n",
    "\n",
    "**Accuracy (1-5):**\n",
    "- 5: Cites primary sources, data is verifiable, peer-reviewed\n",
    "- 3: Makes claims with some supporting links, but not rigorous\n",
    "- 1: No citations, unverifiable claims, or contradicted by reliable sources\n",
    "\n",
    "**Purpose (1-5):**\n",
    "- 5: Educational, informational, no commercial motive\n",
    "- 3: Some bias but primarily informative\n",
    "- 1: Selling something, affiliate links, rage-bait, or propaganda\n",
    "\n",
    "Also check: Is the URL accessible?\n",
    "\n",
    "BE CRITICAL. A typical personal blog with no credentials should score 2-2.5 overall. Only authoritative, well-sourced content deserves 4+.\n",
    "\n",
    "Return JSON only:\n",
    "{{\n",
    "  \"url_accessible\": true/false/null,\n",
    "  \"currency\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"relevance\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"authority\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"accuracy\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"purpose\": {{\"score\": 1-5, \"evidence\": \"...\"}},\n",
    "  \"overall_score\": 1.0-5.0,\n",
    "  \"red_flags\": [\"list\", \"any\", \"concerns\"],\n",
    "  \"summary\": \"One critical sentence\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def verify_reference(reference: dict, model: str = \"gemini-2.0-flash-exp\") -> tuple[dict, LLMLog]:\n",
    "    \"\"\"Verify a single reference using search. Strict scoring.\"\"\"\n",
    "    \n",
    "    prompt = CRAAP_PROMPT_STRICT.format(reference_json=json.dumps(reference, indent=2))\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(\n",
    "            tools=[types.Tool(google_search=types.GoogleSearch())]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    latency_ms = int((time.time() - start_time) * 1000)\n",
    "    \n",
    "    usage = response.usage_metadata\n",
    "    log = LLMLog(\n",
    "        timestamp=datetime.now().isoformat(),\n",
    "        model=model,\n",
    "        task='verify_reference',\n",
    "        prompt_tokens=usage.prompt_token_count,\n",
    "        completion_tokens=usage.candidates_token_count,\n",
    "        total_tokens=usage.total_token_count,\n",
    "        latency_ms=latency_ms\n",
    "    )\n",
    "    llm_logs.append(log)\n",
    "    \n",
    "    try:\n",
    "        clean_text = response.text.strip()\n",
    "        if clean_text.startswith(\"```json\"):\n",
    "            clean_text = clean_text[7:]\n",
    "        if clean_text.startswith(\"```\"):\n",
    "            clean_text = clean_text[3:]\n",
    "        if clean_text.endswith(\"```\"):\n",
    "            clean_text = clean_text[:-3]\n",
    "        \n",
    "        return json.loads(clean_text.strip()), log\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error: {e}\\nRaw: {response.text}\")\n",
    "        return {}, log\n",
    "\n",
    "print(\"✅ verify_reference defined (strict mode)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ef965e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== STAGE 2: VERIFICATION ===\n",
      "[1/15] iTnews\n",
      "       CRAAP: 3.8/5 | 5375ms\n",
      "[2/15] Substack\n",
      "       CRAAP: 3.0/5 | 5842ms\n",
      "[3/15] The Conversation\n",
      "       CRAAP: 3.8/5 | 4127ms\n",
      "[4/15] Windows Forum\n",
      "       CRAAP: 2.4/5 | 4573ms\n",
      "[5/15] Deloitte\n",
      "       CRAAP: 4.0/5 | 5161ms\n",
      "[6/15] IT Brief Australia\n",
      "       CRAAP: 3.8/5 | 5677ms\n",
      "[7/15] PPC Land\n",
      "       CRAAP: 3.8/5 | 4699ms\n",
      "[8/15] The Missing Link\n",
      "       CRAAP: 3.8/5 | 4445ms\n",
      "[9/15] Digital\n",
      "       CRAAP: 4.2/5 | 5466ms\n",
      "[10/15] CFO Dive\n",
      "       CRAAP: 3.8/5 | 4683ms\n",
      "[11/15] Appinventiv\n",
      "       CRAAP: 1.6/5 | 4935ms\n",
      "[12/15] PitchBook\n"
     ]
    },
    {
     "ename": "ClientError",
     "evalue": "429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota. Please migrate to Gemini 2.5 Flash Image (models/gemini-2.5-flash-image) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_requests_per_model', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-exp'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mClientError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, ref \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(refs):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref.get(\u001b[33m'\u001b[39m\u001b[33mplatform_name\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39mref.get(\u001b[33m'\u001b[39m\u001b[33murl\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33munknown\u001b[39m\u001b[33m'\u001b[39m)[:\u001b[32m30\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     result, log = \u001b[43mverify_reference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[32m      8\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m       CRAAP: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.get(\u001b[33m'\u001b[39m\u001b[33moverall_score\u001b[39m\u001b[33m'\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33mN/A\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/5 | \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlog.latency_ms\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 66\u001b[39m, in \u001b[36mverify_reference\u001b[39m\u001b[34m(reference, model)\u001b[39m\n\u001b[32m     62\u001b[39m prompt = CRAAP_PROMPT_STRICT.format(reference_json=json.dumps(reference, indent=\u001b[32m2\u001b[39m))\n\u001b[32m     64\u001b[39m start_time = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGenerateContentConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgoogle_search\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGoogleSearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m latency_ms = \u001b[38;5;28mint\u001b[39m((time.time() - start_time) * \u001b[32m1000\u001b[39m)\n\u001b[32m     76\u001b[39m usage = response.usage_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/models.py:5215\u001b[39m, in \u001b[36mModels.generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   5213\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m remaining_remote_calls_afc > \u001b[32m0\u001b[39m:\n\u001b[32m   5214\u001b[39m   i += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m5215\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_content\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5216\u001b[39m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparsed_config\u001b[49m\n\u001b[32m   5217\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5219\u001b[39m   function_map = _extra_utils.get_function_map(parsed_config)\n\u001b[32m   5220\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m function_map:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/models.py:3997\u001b[39m, in \u001b[36mModels._generate_content\u001b[39m\u001b[34m(self, model, contents, config)\u001b[39m\n\u001b[32m   3994\u001b[39m request_dict = _common.convert_to_dict(request_dict)\n\u001b[32m   3995\u001b[39m request_dict = _common.encode_unserializable_types(request_dict)\n\u001b[32m-> \u001b[39m\u001b[32m3997\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_api_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpost\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m   4002\u001b[39m     config, \u001b[33m'\u001b[39m\u001b[33mshould_return_http_response\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4003\u001b[39m ):\n\u001b[32m   4004\u001b[39m   return_value = types.GenerateContentResponse(sdk_http_response=response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/_api_client.py:1375\u001b[39m, in \u001b[36mBaseApiClient.request\u001b[39m\u001b[34m(self, http_method, path, request_dict, http_options)\u001b[39m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrequest\u001b[39m(\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1367\u001b[39m     http_method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1370\u001b[39m     http_options: Optional[HttpOptionsOrDict] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1371\u001b[39m ) -> SdkHttpResponse:\n\u001b[32m   1372\u001b[39m   http_request = \u001b[38;5;28mself\u001b[39m._build_request(\n\u001b[32m   1373\u001b[39m       http_method, path, request_dict, http_options\n\u001b[32m   1374\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m   response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m   response_body = (\n\u001b[32m   1377\u001b[39m       response.response_stream[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m response.response_stream \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   1378\u001b[39m   )\n\u001b[32m   1379\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m SdkHttpResponse(headers=response.headers, body=response_body)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/_api_client.py:1211\u001b[39m, in \u001b[36mBaseApiClient._request\u001b[39m\u001b[34m(self, http_request, http_options, stream)\u001b[39m\n\u001b[32m   1208\u001b[39m     retry = tenacity.Retrying(**retry_kwargs)\n\u001b[32m   1209\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m retry(\u001b[38;5;28mself\u001b[39m._request_once, http_request, stream)  \u001b[38;5;66;03m# type: ignore[no-any-return]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_request_once\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:477\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    475\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m477\u001b[39m     do = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43miter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:378\u001b[39m, in \u001b[36mBaseRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    376\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     result = \u001b[43maction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:420\u001b[39m, in \u001b[36mBaseRetrying._post_stop_check_actions.<locals>.exc_check\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    418\u001b[39m retry_exc = \u001b[38;5;28mself\u001b[39m.retry_error_cls(fut)\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.reraise:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mretry_exc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    421\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m retry_exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfut\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexception\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:187\u001b[39m, in \u001b[36mRetryError.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mreraise\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> t.NoReturn:\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.last_attempt.failed:\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlast_attempt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/tenacity/__init__.py:480\u001b[39m, in \u001b[36mRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    478\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    479\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m480\u001b[39m         result = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    482\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/_api_client.py:1188\u001b[39m, in \u001b[36mBaseApiClient._request_once\u001b[39m\u001b[34m(self, http_request, stream)\u001b[39m\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1181\u001b[39m   response = \u001b[38;5;28mself\u001b[39m._httpx_client.request(\n\u001b[32m   1182\u001b[39m       method=http_request.method,\n\u001b[32m   1183\u001b[39m       url=http_request.url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1186\u001b[39m       timeout=http_request.timeout,\n\u001b[32m   1187\u001b[39m   )\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m   \u001b[43merrors\u001b[49m\u001b[43m.\u001b[49m\u001b[43mAPIError\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1189\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m HttpResponse(\n\u001b[32m   1190\u001b[39m       response.headers, response \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;28;01melse\u001b[39;00m [response.text]\n\u001b[32m   1191\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/errors.py:121\u001b[39m, in \u001b[36mAPIError.raise_for_response\u001b[39m\u001b[34m(cls, response)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    119\u001b[39m   response_json = response.body_segments[\u001b[32m0\u001b[39m].get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m, {})\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraise_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/google/genai/errors.py:146\u001b[39m, in \u001b[36mAPIError.raise_error\u001b[39m\u001b[34m(cls, status_code, response_json, response)\u001b[39m\n\u001b[32m    132\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Raises an appropriate APIError subclass based on the status code.\u001b[39;00m\n\u001b[32m    133\u001b[39m \n\u001b[32m    134\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m \u001b[33;03m  APIError: For other error status codes.\u001b[39;00m\n\u001b[32m    144\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[32m400\u001b[39m <= status_code < \u001b[32m500\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ClientError(status_code, response_json, response)\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[32m500\u001b[39m <= status_code < \u001b[32m600\u001b[39m:\n\u001b[32m    148\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ServerError(status_code, response_json, response)\n",
      "\u001b[31mClientError\u001b[39m: 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota. Please migrate to Gemini 2.5 Flash Image (models/gemini-2.5-flash-image) for higher quota limits. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_requests_per_model', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.0-flash-exp'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '0s'}]}}"
     ]
    }
   ],
   "source": [
    "# Stage 2: Verify try 2\n",
    "print(\"=== STAGE 2: VERIFICATION ===\")\n",
    "verified = []\n",
    "for i, ref in enumerate(refs):\n",
    "    print(f\"[{i+1}/{len(refs)}] {ref.get('platform_name', ref.get('url', 'unknown')[:30])}\")\n",
    "    result, log = verify_reference(ref)\n",
    "    if result:\n",
    "        print(f\"       CRAAP: {result.get('overall_score', 'N/A')}/5 | {log.latency_ms}ms\")\n",
    "    verified.append({\"reference\": ref, \"craap\": result})\n",
    "\n",
    "print(\"\\n✅ Verification complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900a37f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LLMLog(timestamp='2026-01-08T03:07:54.437699', model='gemini-2.0-flash-exp', task='extract_references', prompt_tokens=3667, completion_tokens=1816, total_tokens=5483, latency_ms=11000),\n",
       " LLMLog(timestamp='2026-01-08T03:08:00.395690', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=404, completion_tokens=376, total_tokens=780, latency_ms=5957),\n",
       " LLMLog(timestamp='2026-01-08T03:08:05.113982', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=380, completion_tokens=421, total_tokens=801, latency_ms=4718),\n",
       " LLMLog(timestamp='2026-01-08T03:08:09.161386', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=389, completion_tokens=331, total_tokens=720, latency_ms=4047),\n",
       " LLMLog(timestamp='2026-01-08T03:08:14.158766', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=389, completion_tokens=317, total_tokens=706, latency_ms=4997),\n",
       " LLMLog(timestamp='2026-01-08T03:08:20.716454', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=380, completion_tokens=540, total_tokens=920, latency_ms=6557),\n",
       " LLMLog(timestamp='2026-01-08T03:08:27.668563', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=388, completion_tokens=517, total_tokens=905, latency_ms=6951),\n",
       " LLMLog(timestamp='2026-01-08T03:08:34.448232', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=386, completion_tokens=372, total_tokens=758, latency_ms=6779),\n",
       " LLMLog(timestamp='2026-01-08T03:08:39.370975', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=377, completion_tokens=361, total_tokens=738, latency_ms=4922),\n",
       " LLMLog(timestamp='2026-01-08T03:08:45.260382', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=391, completion_tokens=496, total_tokens=887, latency_ms=5889),\n",
       " LLMLog(timestamp='2026-01-08T03:08:51.600500', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=392, completion_tokens=431, total_tokens=823, latency_ms=6339),\n",
       " LLMLog(timestamp='2026-01-08T03:08:57.426683', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=383, completion_tokens=410, total_tokens=793, latency_ms=5825),\n",
       " LLMLog(timestamp='2026-01-08T03:09:02.845932', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=369, completion_tokens=436, total_tokens=805, latency_ms=5419),\n",
       " LLMLog(timestamp='2026-01-08T03:09:07.862328', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=359, completion_tokens=419, total_tokens=778, latency_ms=5016),\n",
       " LLMLog(timestamp='2026-01-08T03:09:13.057078', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=376, completion_tokens=357, total_tokens=733, latency_ms=5194),\n",
       " LLMLog(timestamp='2026-01-08T03:09:18.248999', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=390, completion_tokens=419, total_tokens=809, latency_ms=5191),\n",
       " LLMLog(timestamp='2026-01-08T03:18:09.646011', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=737, completion_tokens=364, total_tokens=1101, latency_ms=5375),\n",
       " LLMLog(timestamp='2026-01-08T03:18:15.488329', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=713, completion_tokens=475, total_tokens=1188, latency_ms=5842),\n",
       " LLMLog(timestamp='2026-01-08T03:18:19.616335', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=722, completion_tokens=333, total_tokens=1055, latency_ms=4127),\n",
       " LLMLog(timestamp='2026-01-08T03:18:24.190109', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=722, completion_tokens=350, total_tokens=1072, latency_ms=4573),\n",
       " LLMLog(timestamp='2026-01-08T03:18:29.352287', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=713, completion_tokens=370, total_tokens=1083, latency_ms=5161),\n",
       " LLMLog(timestamp='2026-01-08T03:18:35.029738', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=721, completion_tokens=399, total_tokens=1120, latency_ms=5677),\n",
       " LLMLog(timestamp='2026-01-08T03:18:39.729113', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=719, completion_tokens=367, total_tokens=1086, latency_ms=4699),\n",
       " LLMLog(timestamp='2026-01-08T03:18:44.174818', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=710, completion_tokens=342, total_tokens=1052, latency_ms=4445),\n",
       " LLMLog(timestamp='2026-01-08T03:18:49.641696', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=724, completion_tokens=432, total_tokens=1156, latency_ms=5466),\n",
       " LLMLog(timestamp='2026-01-08T03:18:54.325037', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=725, completion_tokens=342, total_tokens=1067, latency_ms=4683),\n",
       " LLMLog(timestamp='2026-01-08T03:18:59.261163', model='gemini-2.0-flash-exp', task='verify_reference', prompt_tokens=716, completion_tokens=364, total_tokens=1080, latency_ms=4935)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0eb02658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ calculate_snifftest defined\n"
     ]
    }
   ],
   "source": [
    "# how to aggregate\n",
    "def calculate_snifftest(verified_refs: list[dict]) -> dict:\n",
    "    \"\"\"Calculate Snifftest score from verified references.\"\"\"\n",
    "    \n",
    "    scores = []\n",
    "    red_flags = []\n",
    "    \n",
    "    for v in verified_refs:\n",
    "        craap = v.get(\"craap\", {})\n",
    "        if craap and craap.get(\"overall_score\"):\n",
    "            scores.append(craap[\"overall_score\"])\n",
    "        if craap and craap.get(\"red_flags\"):\n",
    "            red_flags.extend(craap[\"red_flags\"])\n",
    "    \n",
    "    if not scores:\n",
    "        return {\n",
    "            \"score\": 0,\n",
    "            \"label\": \"Unknown\",\n",
    "            \"emoji\": \"❓\",\n",
    "            \"scores\": scores,\n",
    "            \"red_flags\": red_flags\n",
    "        }\n",
    "    \n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    min_score = min(scores)\n",
    "    low_count = sum(1 for s in scores if s < 2.5)\n",
    "    \n",
    "    # Determine label (penalize if ANY source is bad)\n",
    "    if min_score < 2 or low_count >= 2:\n",
    "        label, emoji = \"Foul\", \"🤢\"\n",
    "    elif mean_score < 2.5 or low_count >= 1:\n",
    "        label, emoji = \"Funky\", \"😬\"\n",
    "    elif mean_score < 3.5:\n",
    "        label, emoji = \"Fresh\", \"😊\"\n",
    "    else:\n",
    "        label, emoji = \"Sweet\", \"🌟\"\n",
    "    \n",
    "    return {\n",
    "        \"score\": round(mean_score, 2),\n",
    "        \"label\": label,\n",
    "        \"emoji\": emoji,\n",
    "        \"min_score\": min_score,\n",
    "        \"num_refs\": len(scores),\n",
    "        \"low_quality_count\": low_count,\n",
    "        \"red_flags\": list(set(red_flags))  # dedupe\n",
    "    }\n",
    "\n",
    "print(\"✅ calculate_snifftest defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20bf7744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ generate_snifftest_summary defined\n"
     ]
    }
   ],
   "source": [
    "SNIFFTEST_SUMMARY_PROMPT = \"\"\"\n",
    "You are the Snifftest summarizer. Given CRAAP evaluations of all sources in an AI-generated response, write a blunt, honest summary.\n",
    "\n",
    "SNIFFTEST RESULT: {label} {emoji} ({score}/5)\n",
    "\n",
    "VERIFIED SOURCES:\n",
    "{verified_json}\n",
    "\n",
    "Write a 2-3 sentence summary that:\n",
    "1. States the overall verdict clearly\n",
    "2. Calls out the weakest sources specifically\n",
    "3. Notes any red flags\n",
    "4. Suggests what would improve the response (if applicable)\n",
    "\n",
    "Be direct and a little snarky. Examples:\n",
    "- \"This response smells funky. Half the sources are random blogs with no credentials...\"\n",
    "- \"Pretty sweet! All sources are peer-reviewed or from reputable institutions...\"\n",
    "- \"Foul. The main claim relies entirely on a 2019 Medium post by someone called 'CryptoGuru'...\"\n",
    "\n",
    "Keep it under 50 words. No JSON, just plain text.\n",
    "\"\"\"\n",
    "\n",
    "def generate_snifftest_summary(verified_refs: list[dict], snifftest: dict) -> tuple[str, LLMLog]:\n",
    "    \"\"\"Generate human-readable Snifftest summary.\"\"\"\n",
    "    \n",
    "    prompt = SNIFFTEST_SUMMARY_PROMPT.format(\n",
    "        label=snifftest[\"label\"],\n",
    "        emoji=snifftest[\"emoji\"],\n",
    "        score=snifftest[\"score\"],\n",
    "        verified_json=json.dumps(verified_refs, indent=2)\n",
    "    )\n",
    "    \n",
    "    summary, log = generate_with_logging(prompt)\n",
    "    return summary.strip(), log\n",
    "\n",
    "print(\"✅ generate_snifftest_summary defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0427d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code has not been executed, provided as a reference.\n",
    "def run_snifftest(answer: str, verbose: bool = True) -> dict:\n",
    "    \"\"\"Run full Snifftest pipeline on an AI-generated answer.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"input\": answer,\n",
    "        \"references\": [],\n",
    "        \"verified\": [],\n",
    "        \"snifftest\": {},\n",
    "        \"summary\": \"\",\n",
    "        \"logs\": []\n",
    "    }\n",
    "    \n",
    "    # Stage 1: Extract\n",
    "    if verbose:\n",
    "        print(\"=== STAGE 1: EXTRACTION ===\")\n",
    "    refs, extract_log = extract_references(answer)\n",
    "    results[\"references\"] = refs\n",
    "    results[\"logs\"].append({\"stage\": \"extract\", **extract_log.to_dict()})\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Found {len(refs)} references ({extract_log.total_tokens} tokens, {extract_log.latency_ms}ms)\\n\")\n",
    "    \n",
    "    if not refs:\n",
    "        results[\"summary\"] = \"No references found to evaluate.\"\n",
    "        return results\n",
    "    \n",
    "    # Stage 2: Verify each\n",
    "    if verbose:\n",
    "        print(\"=== STAGE 2: VERIFICATION ===\")\n",
    "    \n",
    "    for i, ref in enumerate(refs):\n",
    "        name = ref.get('platform_name') or ref.get('url', 'unknown')[:40]\n",
    "        if verbose:\n",
    "            print(f\"[{i+1}/{len(refs)}] {name}...\", end=\" \")\n",
    "        \n",
    "        craap, verify_log = verify_reference(ref)\n",
    "        results[\"verified\"].append({\"reference\": ref, \"craap\": craap})\n",
    "        results[\"logs\"].append({\"stage\": f\"verify_{i+1}\", **verify_log.to_dict()})\n",
    "        \n",
    "        if verbose and craap:\n",
    "            score = craap.get('overall_score', '?')\n",
    "            print(f\"CRAAP: {score}/5\")\n",
    "    \n",
    "    # Stage 3: Calculate Snifftest\n",
    "    if verbose:\n",
    "        print(\"\\n=== STAGE 3: SNIFFTEST ===\")\n",
    "    \n",
    "    snifftest = calculate_snifftest(results[\"verified\"])\n",
    "    results[\"snifftest\"] = snifftest\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"{snifftest['emoji']} {snifftest['label']}: {snifftest['score']}/5\")\n",
    "        print(f\"   {snifftest['num_refs']} sources, {snifftest['low_quality_count']} low quality\")\n",
    "    \n",
    "    # Stage 4: Generate summary\n",
    "    if verbose:\n",
    "        print(\"\\n=== STAGE 4: SUMMARY ===\")\n",
    "    \n",
    "    summary, summary_log = generate_snifftest_summary(results[\"verified\"], snifftest)\n",
    "    results[\"summary\"] = summary\n",
    "    results[\"logs\"].append({\"stage\": \"summary\", **summary_log.to_dict()})\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\n{snifftest['emoji']} SNIFFTEST: {snifftest['label'].upper()} ({snifftest['score']}/5)\")\n",
    "        print(\"-\" * 50)\n",
    "        print(summary)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"✅ run_snifftest pipeline ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d66ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_snifftest(example_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DETAILED CRAAP SCORES ===\\n\")\n",
    "for v in results[\"verified\"]:\n",
    "    ref = v[\"reference\"]\n",
    "    craap = v[\"craap\"]\n",
    "    \n",
    "    name = ref.get(\"platform_name\") or ref.get(\"url\", \"unknown\")[:40]\n",
    "    print(f\"📎 {name}\")\n",
    "    print(f\"   Type: {ref.get('source_type', 'unknown')}\")\n",
    "    \n",
    "    if craap:\n",
    "        print(f\"   C:{craap.get('currency',{}).get('score','?')} \"\n",
    "              f\"R:{craap.get('relevance',{}).get('score','?')} \"\n",
    "              f\"A:{craap.get('authority',{}).get('score','?')} \"\n",
    "              f\"A:{craap.get('accuracy',{}).get('score','?')} \"\n",
    "              f\"P:{craap.get('purpose',{}).get('score','?')} \"\n",
    "              f\"→ {craap.get('overall_score','?')}/5\")\n",
    "        if craap.get(\"red_flags\"):\n",
    "            print(f\"   🚩 {', '.join(craap['red_flags'])}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a512884",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_tokens = sum(log.get(\"total_tokens\", 0) for log in results[\"logs\"])\n",
    "total_latency = sum(log.get(\"latency_ms\", 0) for log in results[\"logs\"])\n",
    "\n",
    "print(f\"=== PIPELINE COST ===\")\n",
    "print(f\"Total tokens: {total_tokens:,}\")\n",
    "print(f\"Total latency: {total_latency:,}ms ({total_latency/1000:.1f}s)\")\n",
    "print(f\"Calls: {len(results['logs'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
